{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Network Training \n",
        "This tutorial demonstrates how to train the LSTM-based model for the core loss prediction. The network model will be trained based on Dataset_full.json and saved as a state dictionary (.sd) file.\n"
      ],
      "metadata": {
        "id": "JRKtl4ciW12L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Import Packages\n",
        "In this demo, the neural network is synthesized using the PyTorch framework. Please install PyTorch according to the [official guidance](https://pytorch.org/get-started/locally/) , then import PyTorch and other dependent modules."
      ],
      "metadata": {
        "id": "GQzQz2mkMqL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6IdDUEup6490"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Define Network Structure\n",
        "In this part, we define the structure of the LSTM neural network. Refer to the [PyTorch document](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) for more details."
      ],
      "metadata": {
        "id": "r9uAIku9M0Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model structures and functions\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(1, 18, num_layers=1, batch_first=True, bidirectional=False)\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(18, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :] # Get last output only (many-to-one)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "        \n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "q5LA1kae860Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load the Dataset\n",
        "In this part, we load and pre-process the dataset for the network training and testing. In this demo, a small dataset containing sinusoidal waveforms measured with N87 ferrite material under different frequency, and flux density is used, which can be downloaded from the [MagNet GitHub](https://github.com/PrincetonUniversity/Magnet) repository under \"tutorial\". "
      ],
      "metadata": {
        "id": "HxpYPTvPM6eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "\n",
        "def get_dataset(data_length=1000):\n",
        "    # Load .json Files\n",
        "    with open('/content/Dataset_full.json','r') as load_f:\n",
        "        DATA = json.load(load_f)\n",
        "    \n",
        "    Seq = DATA['Sequence']\n",
        "    Seq = np.array(Seq)\n",
        "    Seq = Seq[:,:data_length]\n",
        "    Power = DATA['Power_Loss']\n",
        "    Power = np.log10(Power)\n",
        "\n",
        "    print(np.shape(Seq))\n",
        "    print(np.shape(Power))\n",
        "    \n",
        "    in_tensors = torch.from_numpy(Seq).view(-1, data_length, 1)\n",
        "    out_tensors = torch.from_numpy(Power).view(-1, 1)\n",
        "\n",
        "    # # Save dataset for future use\n",
        "    # np.save(\"dataset.fc.in.npy\", in_tensors.numpy())\n",
        "    # np.save(\"dataset.fc.out.npy\", out_tensors.numpy())\n",
        "\n",
        "    return torch.utils.data.TensorDataset(in_tensors, out_tensors)\n"
      ],
      "metadata": {
        "id": "Y-6sTpOoAUWZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Training and Testing the Model\n",
        "In this part, we program the training and testing procedure of the network model. The loaded dataset is randomly split into training set, validation set, and test set. The output of the training is the state dictionary file (.sd) containing all the trained parameter values."
      ],
      "metadata": {
        "id": "1HRhRbOaM_ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Config the model training\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Reproducibility\n",
        "    random.seed(1)\n",
        "    np.random.seed(1)\n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    NUM_EPOCH = 500\n",
        "    BATCH_SIZE = 256\n",
        "    DECAY_EPOCH = 100\n",
        "    DECAY_RATIO = 0.5\n",
        "    LR_INI = 0.02\n",
        "\n",
        "    # Select GPU as default device\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = get_dataset()\n",
        "\n",
        "    # Split the dataset\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    valid_size = len(dataset) - train_size\n",
        "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "    kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "\n",
        "    train_size = int(0.6 * len(dataset))\n",
        "    valid_size = int(0.2 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - valid_size\n",
        "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "    kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "\n",
        "    # Setup network\n",
        "    net = Net().double().to(device)\n",
        "\n",
        "    # Log the number of parameters\n",
        "    print(\"Number of parameters: \", count_parameters(net))\n",
        "\n",
        "    # Setup optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LR_INI) \n",
        "\n",
        "    # Train the network\n",
        "    for epoch_i in range(NUM_EPOCH):\n",
        "\n",
        "        # Train for one epoch\n",
        "        epoch_train_loss = 0\n",
        "        net.train()\n",
        "        optimizer.param_groups[0]['lr'] = LR_INI* (DECAY_RATIO ** (0+ epoch_i // DECAY_EPOCH))\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs.to(device))\n",
        "            loss = criterion(outputs, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Compute Validation Loss\n",
        "        with torch.no_grad():\n",
        "            epoch_valid_loss = 0\n",
        "            for inputs, labels in valid_loader:\n",
        "                outputs = net(inputs.to(device))\n",
        "                loss = criterion(outputs, labels.to(device))\n",
        "\n",
        "                epoch_valid_loss += loss.item()\n",
        "        \n",
        "        if (epoch_i+1)%100 == 0:\n",
        "          print(f\"Epoch {epoch_i+1:2d} \"\n",
        "              f\"Train {epoch_train_loss / len(train_dataset) * 1e5:.5f} \"\n",
        "              f\"Valid {epoch_valid_loss / len(valid_dataset) * 1e5:.5f}\")\n",
        "        \n",
        "    # Save the model parameters\n",
        "    torch.save(net.state_dict(), \"/content/Model_FNN.sd\")\n",
        "    print(\"Training finished! Model is saved!\")\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    net.eval()\n",
        "    y_meas = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            y_pred.append(net(inputs.to(device)))\n",
        "            y_meas.append(labels.to(device))\n",
        "\n",
        "    y_meas = torch.cat(y_meas, dim=0)\n",
        "    y_pred = torch.cat(y_pred, dim=0)\n",
        "    print(f\"Test Loss: {F.mse_loss(y_meas, y_pred).item() / len(test_dataset) * 1e5:.5f}\")\n",
        "\n",
        "    yy_pred = 10**(y_pred.cpu().numpy())\n",
        "    yy_meas = 10**(y_meas.cpu().numpy())\n",
        "    \n",
        "    # Relative Error\n",
        "    Error_re = abs(yy_pred-yy_meas)/abs(yy_meas)*100\n",
        "    Error_re_avg = np.mean(Error_re)\n",
        "    Error_re_rms = np.sqrt(np.mean(Error_re ** 2))\n",
        "    Error_re_max = np.max(Error_re)\n",
        "    print(f\"Relative Error: {Error_re_avg:.8f}\")\n",
        "    print(f\"RMS Error: {Error_re_rms:.8f}\")\n",
        "    print(f\"MAX Error: {Error_re_max:.8f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ndXYTq9b9R",
        "outputId": "bb665f31-c601-4826-8179-87a73f0abc4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1422, 1000)\n",
            "(1422,)\n",
            "Number of parameters:  2065\n",
            "Epoch 100 Train 12.34538 Valid 18.47287\n",
            "Epoch 200 Train 2.11043 Valid 4.66952\n",
            "Epoch 300 Train 0.41907 Valid 1.07567\n",
            "Epoch 400 Train 0.29561 Valid 0.75741\n",
            "Epoch 500 Train 0.20251 Valid 0.61505\n",
            "Training finished! Model is saved!\n",
            "Test Loss: 0.70196\n",
            "Relative Error: 5.36657774\n",
            "RMS Error: 9.02621915\n",
            "MAX Error: 66.53191345\n"
          ]
        }
      ]
    }
  ]
}